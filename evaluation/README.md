# Model Evaluation ðŸ“Š

This folder contains scripts and instructions for evaluating the object detection models (YOLOv7, YOLOv11, RT-DETR) using both COCO-style metrics and frame-level ROC/AUC metrics.

## 1. Overview

Evaluation is performed on two levels:

### 1.1 COCO-based evaluation
We use a modified COCO evaluation script to calculate detection metrics specifically for polyps:

- **Script:** `cocoeval.py`
- **Origin:** This file is based on the [COCO API](https://github.com/cocodataset/cocoapi) and adapted by [Cosmo Intelligent Medical Devices](https://github.com/cosmoimd/yolov7/tree/main).
- **Purpose:** Compute standard COCO metrics (AP, AR) and area-based metrics for small, medium, and large polyps.

For each model, run the evaluation using the predictions generated by the model's `test.py`:
```bash
python cocoeval.py --gt <coco_gt.json> --pred <predictions.json>
```

### 1.2 Frame-level evaluation
To complement COCO metrics, we provide frame-level ROC/AUC analysis:
- Script: ```bash roc_universal.py```
- Usage:
```bash
python roc_universal.py <coco_gt.json> <predictions.json> <output_dir>
```
This script will:
- Build frame-level labels and scores using IoU â‰¥ 0.2
- Save labels and scores as `labels_scores.pkl`
- Generate a ROC curve plot (`ROC_curve.png`)
- Print AUC value

### 2. Additional Notes
- Ensure predictions are in **COCO detection format**.  
- The `roc_universal.py` script automatically handles frames without any ground-truth polyps.  
- Frame-level metrics are complementary to COCO metrics, providing insight into practical polyp detection per video frame.  
- You can adjust the IoU threshold by modifying the `iou_thr` argument in `build_frame_scores` if needed.  
